{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Podcast Video from Transcript - V1 (Final Step)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook outlines the final step in generating a podcast video from a PDF document. It takes the podcast transcript and audio (generated in previous steps) and creates a video using the Wan2.1 1.3B model.\n",
        "\n",
        "## Key Points\n",
        "\n",
        "1. **Last Step:** This is the final step in the \"Generate Podcast from PDF V1\" series, where we generate a video from the transcript data.\n",
        "2. **Model:** We utilize the Wan2.1 1.3B model for video generation.\n",
        "3. **Data:**\n",
        "   - Podcast transcript (generated from the previous step)\n",
        "   - Podcast audio (generated from a previous step)\n",
        "4. **Groq API Key:** A Groq API key is required for prompt generation. You can store it securely in Google Colab user data using `google.colab.userdata.get(\"grpq\")`.\n",
        "5. **Hardware:** This notebook is designed to run on an A100 GPU (40) with 64GB of RAM. You might be able to run it on a Colab Pro T4 with 24GB of RAM by adjusting memory settings, but performance may vary.\n",
        "\n",
        "## Steps\n",
        "\n",
        "1. **Install Libraries:** Install necessary libraries, including `diffusers`, `moviepy`, `langchain-groq`, and others.\n",
        "2. **Load Data:** Load the podcast transcript and audio data.\n",
        "3. **Prompt Generation:** Use Langchain and Groq to generate creative prompts for video generation based on the transcript.\n",
        "4. **Video Generation:** Utilize the Wan2.1 1.3B model with the generated prompts to create video clips.\n",
        "5. **Combine Clips:** Merge the individual video clips and audio to create the final podcast video.\n",
        "\n",
        "## Usage\n",
        "\n",
        "1. Provide the path to your podcast transcript and audio files.\n",
        "2. Set your Groq API key using `google.colab.userdata.set(\"grpq\", \"YOUR_API_KEY\")`.\n",
        "3. Execute the notebook cells to generate the podcast video.\n",
        "\n",
        "## Notes\n",
        "\n",
        "- Make sure to enable GPU acceleration in Colab.\n",
        "- Adjust video generation parameters like `height`, `width`, and `guidance_scale` as needed.\n",
        "- The checkpoint file allows resuming the generation process if interrupted."
      ],
      "metadata": {
        "id": "RCIlPmDb6s2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cloning Wan 2.1 repo for requirement.txt file\n",
        "!git clone https://github.com/Wan-Video/Wan2.1.git"
      ],
      "metadata": {
        "id": "BBJ0MtQf8JgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## installing all dependiencies\n",
        "!pip install -r Wan2.1/requirements.txt\n",
        "!pip install --upgrade diffusers[torch]\n",
        "!pip install moviepy diffusers accelerate --quiet\n",
        "!apt-get install -y ffmpeg\n",
        "!pip install langchain-groq"
      ],
      "metadata": {
        "id": "S2AYMFpYEBPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Transcript (generate in previous step) and common import and consts."
      ],
      "metadata": {
        "id": "P-ykwthE9wtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import tempfile\n",
        "from tqdm import tqdm\n",
        "from moviepy.editor import AudioFileClip, ImageClip, concatenate_videoclips\n",
        "from PIL import Image\n",
        "# from diffusers import StableDiffusionPipeline\n",
        "import ast\n",
        "import pickle\n",
        "\n",
        "# Load podcast transcript\n",
        "with open('/content/podcast_ready_data.pkl', 'rb') as file:\n",
        "    PODCAST_TEXT = pickle.load(file)\n",
        "\n",
        "# Use tempfile for output chunks\n",
        "output_dir = tempfile.mkdtemp()\n",
        "\n",
        "# Initialize Stable Diffusion\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "# pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16).to(device)\n",
        "\n",
        "# Load the final audio (generated from your provided code)\n",
        "final_audio_path = \"/content/_podcast.mp3\"  # You should export your `final_audio` to this path\n",
        "\n",
        "segments = ast.literal_eval(PODCAST_TEXT)"
      ],
      "metadata": {
        "id": "O_N-FYh4LNfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function to generate Video using prompt (480p and 720p) use 14B model for high quality output."
      ],
      "metadata": {
        "id": "g8wt5nLa88YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers.utils import export_to_video\n",
        "from diffusers import AutoencoderKLWan, WanPipeline\n",
        "from diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler\n",
        "\n",
        "# Available models: Wan-AI/Wan2.1-T2V-14B-Diffusers, Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n",
        "model_id = \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\"\n",
        "vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n",
        "flow_shift = 3.0 # 5.0 for 720P, 3.0 for 480P\n",
        "scheduler = UniPCMultistepScheduler(prediction_type='flow_prediction', use_flow_sigmas=True, num_train_timesteps=1000, flow_shift=flow_shift)\n",
        "pipe = WanPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.bfloat16)\n",
        "pipe.scheduler = scheduler\n",
        "pipe.to(device)\n",
        "\n",
        "def generate_video(prompt, negative_prompt,num_frames,output_dir,output_file, height=480,width=960, guidance_scale=5.0):\n",
        "    output = pipe(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        num_frames=num_frames,\n",
        "        guidance_scale=guidance_scale,\n",
        "    ).frames[0]\n",
        "    export_to_video(output, f\"{output_dir}/{output_file}\", fps=16)\n",
        "    return f\"{output_dir}/{output_file}\"\n"
      ],
      "metadata": {
        "id": "eWlsPnQKSqdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window.\"\n",
        "negative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n",
        "generate_video(prompt,negative_prompt,88,\"/content\",\"dog_cat_cook.mp4\")"
      ],
      "metadata": {
        "id": "-HL-kYUn00mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function to generate Prompt from transcrip to generate video\n",
        "*** you can more fine tune the prompt as needed ***"
      ],
      "metadata": {
        "id": "YhCPXARw_IRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "model = 'llama3-70b-8192'\n",
        "\n",
        "chat = ChatGroq(temperature=.5, groq_api_key=userdata.get(\"grpq\"), model_name=model)\n",
        "\n",
        "\n",
        "def get_prompt_to_generate_video_clip(speaker, text):\n",
        "  SYSTEM_PROMPT = \"\"\"\n",
        "    You are a creative content creator and prompt engineer. And you are using a tool to generate the podcast video.\n",
        "    your job is to create prompt for stable diffusions  Wan2 1.3B model in creative way assuming we provide you the transcript of speaker 1 and speaker 2 format.\n",
        "    so create creative prompts by analyzing the transcript so it can get more engagements on social media.\n",
        "    return the prompt in string format that only contains the prompt and negative_prompt within a JSON structure like this it should not contain anything else other then JSON:\n",
        "    {{\n",
        "      \"prompt\":\"A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window.\",\n",
        "      \"negative_prompt\":\"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n",
        "    }}\n",
        "    \"\"\"\n",
        "  human = \"speaker:{speaker} , transcript:{transcript}\"\n",
        "  prompt = ChatPromptTemplate.from_messages([(\"system\", SYSTEM_PROMPT), (\"human\", human)])\n",
        "\n",
        "  chain_for_prompt_gen = prompt | chat\n",
        "\n",
        "  res = chain_for_prompt_gen.invoke({\"speaker\":speaker,\"transcript\":  text})\n",
        "\n",
        "  # Parse the JSON response to access the prompt\n",
        "  import json\n",
        "  try:\n",
        "    prompt_data = json.loads(res.content)\n",
        "    return prompt_data # Handle cases where \"prompt\" might be missing\n",
        "  except json.JSONDecodeError as e:\n",
        "    print(f\"Error decoding JSON: {e}\")\n",
        "    print(f\"Raw response: {res.content}\")\n",
        "    return {\"prompt\":\"\",\"negative_prompt\":\"\"} # Return empty string if JSON decoding fails"
      ],
      "metadata": {
        "id": "BM1pMN4pPm49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_prompt_to_generate_video_clip(segments[0][0],segments[0][1])"
      ],
      "metadata": {
        "id": "Hu2QchHJOLwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Main function to generate Video from transcript segments."
      ],
      "metadata": {
        "id": "AhQP1CZjBNqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import torch\n",
        "import traceback\n",
        "from tqdm import tqdm\n",
        "from moviepy.editor import AudioFileClip, VideoFileClip, concatenate_videoclips\n",
        "\n",
        "# Setup\n",
        "output_dir = \"/content/generated_clips\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "checkpoint_file = \"/content/generation_checkpoint.txt\"\n",
        "final_output_path = \"/content/podcast_video.mp4\"\n",
        "\n",
        "# Load final audio\n",
        "audio_clip = AudioFileClip(final_audio_path)\n",
        "audio_duration = audio_clip.duration\n",
        "\n",
        "# Check where we left off\n",
        "start_index = 0\n",
        "if os.path.exists(checkpoint_file):\n",
        "    with open(checkpoint_file, \"r\") as f:\n",
        "        start_index = int(f.read().strip())\n",
        "\n",
        "print(f\"🔁 Resuming from segment {start_index}\")\n",
        "\n",
        "# Generate video per segment\n",
        "segment_start = 0\n",
        "video_clips = []\n",
        "\n",
        "for i, (speaker, text) in enumerate(tqdm(segments, desc=\"Generating video clips\", unit=\"clip\")):\n",
        "    if i < start_index:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n🎙️ Generating prompt for segment {i}\")\n",
        "        prompt_data = get_prompt_to_generate_video_clip(speaker, text)\n",
        "\n",
        "        if not prompt_data[\"prompt\"]:\n",
        "            raise ValueError(\"Prompt generation failed or returned empty.\")\n",
        "\n",
        "        # Estimate segment duration from audio\n",
        "        segment_duration = max(3, len(text.split()) * 0.3)\n",
        "        if segment_start + segment_duration > audio_duration:\n",
        "            segment_duration = audio_duration - segment_start\n",
        "\n",
        "        print(f\"🕒 Segment duration: {segment_duration:.2f}s\")\n",
        "\n",
        "        # Generate video\n",
        "        clip_name = f\"clip_{i:03d}.mp4\"\n",
        "        clip_path = os.path.join(output_dir, clip_name)\n",
        "        print(f\"🎥 Generating video clip: {clip_name}\")\n",
        "\n",
        "        result_path = generate_video(\n",
        "            prompt=prompt_data[\"prompt\"],\n",
        "            negative_prompt=prompt_data[\"negative_prompt\"],\n",
        "            num_frames=int(segment_duration * 16),  # FPS = 16\n",
        "            output_dir=output_dir,\n",
        "            output_file=clip_name\n",
        "        )\n",
        "\n",
        "        # Attach audio segment\n",
        "        video = VideoFileClip(result_path).set_duration(segment_duration)\n",
        "        audio_segment = audio_clip.subclip(segment_start, segment_start + segment_duration)\n",
        "        video = video.set_audio(audio_segment)\n",
        "\n",
        "        video_clips.append(video)\n",
        "\n",
        "        # Save checkpoint\n",
        "        with open(checkpoint_file, \"w\") as f:\n",
        "            f.write(str(i + 1))\n",
        "\n",
        "        # Cleanup GPU memory\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        segment_start += segment_duration\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to process segment {i}: {e}\")\n",
        "        traceback.print_exc()\n",
        "        break  # Stop so we can resume later from same segment\n",
        "\n",
        "# Finalize video if we have clips\n",
        "if video_clips:\n",
        "    final_video = concatenate_videoclips(video_clips, method=\"compose\")\n",
        "    final_video.write_videofile(final_output_path, fps=24, audio_codec=\"aac\")\n",
        "    print(f\"\\n✅ Final podcast video saved at: {final_output_path}\")\n",
        "else:\n",
        "    print(\"⚠️ No video clips were generated.\")\n"
      ],
      "metadata": {
        "id": "E1RzFhABV3FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fallback logic it Notebook Fails at any point.\n",
        "*** Only run if above cell fails at any iteration (not on 0) :) ***"
      ],
      "metadata": {
        "id": "6f4nEh4gBXn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from moviepy.editor import AudioFileClip, VideoFileClip, concatenate_videoclips\n",
        "\n",
        "# Setup\n",
        "output_dir = \"/content/generated_clips\"\n",
        "final_output_path = \"/content/podcast_video_partial.mp4\"\n",
        "\n",
        "# Load all available video clips\n",
        "clips = sorted([\n",
        "    os.path.join(output_dir, f)\n",
        "    for f in os.listdir(output_dir)\n",
        "    if f.endswith(\".mp4\")\n",
        "])\n",
        "\n",
        "print(f\"✅ Found {len(clips)} video clips.\")\n",
        "\n",
        "video_clips = [VideoFileClip(clip) for clip in clips]\n",
        "\n",
        "# Load full audio\n",
        "audio_clip = AudioFileClip(final_audio_path)\n",
        "\n",
        "# Calculate total duration from segments 0-N\n",
        "total_duration = 0\n",
        "for speaker, text in segments[:13]:  # 0 to N inclusive\n",
        "    words = len(text.split())\n",
        "    segment_duration = max(3, words * 0.3)\n",
        "    total_duration += segment_duration\n",
        "\n",
        "print(f\"🎵 Total duration for audio cut: {total_duration:.2f} seconds\")\n",
        "\n",
        "# Cut the audio to match the video length\n",
        "audio_clip = audio_clip.subclip(0, total_duration)\n",
        "\n",
        "# Concatenate video clips\n",
        "final_video = concatenate_videoclips(video_clips, method=\"compose\")\n",
        "\n",
        "# Set the trimmed audio\n",
        "final_video = final_video.set_audio(audio_clip)\n",
        "\n",
        "# Export the final video\n",
        "final_video.write_videofile(final_output_path, fps=24, audio_codec=\"aac\")\n",
        "\n",
        "print(f\"\\n✅ Final partial podcast video saved at: {final_output_path}\")\n"
      ],
      "metadata": {
        "id": "dPpT_7ATjxlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates a workflow for generating podcast videos from transcripts and audio using the Wan2.1 1.3B model. While the current implementation provides a basic framework, there are several areas for improvement and further exploration.\n",
        "\n",
        "## TODO\n",
        "\n",
        "1. **Fine-tune Prompts:** Experiment with different prompt engineering techniques to generate more engaging and creative video content. Consider using more detailed descriptions, specifying camera angles, or incorporating emotions.\n",
        "\n",
        "2. **Incorporate PDF Content:** Extend the pipeline to analyze the original PDF document, including images and text from specific sections or pages. This would allow for more context-aware video generation and potentially include relevant visuals in the final output.\n",
        "\n",
        "3. **Explore Other Models:** Investigate alternative video generation models like Lightricks/LTX-Video and compare their performance and output quality to Wan2.1 1.3B. This could lead to improved video quality or more diverse creative options.\n",
        "\n",
        "4. **Create a Complete Pipeline:** Develop a streamlined pipeline that takes a PDF document as input and automatically generates a complete podcast video, including transcript extraction, audio generation, and video creation. This would make the process more user-friendly and accessible.\n",
        "\n",
        "5. **Develop API or Gradio App:** Create an API or a Gradio app to expose the functionality of the pipeline to a wider audience. This would allow users to easily generate podcast videos without needing to interact directly with the code.\n",
        "\n",
        "By addressing these TODO items, we can significantly enhance the capabilities of this workflow and create more compelling and informative podcast videos."
      ],
      "metadata": {
        "id": "K_5ACXz6B315"
      }
    }
  ]
}